# ComfyUI Wrapper for ControlNet with Stable Diffusion

## Overview
This project implements a wrapper around ComfyUI to integrate the ControlNet model with Stable Diffusion for video generation. The input is a video file, and the output is a video with frames generated by the AI models.

## Setup

### Prerequisites
- Python 3.8+
- PyTorch

### Installation
1. Clone the repository:
    ```bash
    git clone https://github.com/moradisa/comfyui-wrapper-controlnet.git
    cd comfyui-wrapper-controlnet

    ```

2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

3. Download the Stable Diffusion and ControlNet models:
    - [Stable Diffusion](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/blob/main/sd-v1-4.ckpt)
    - [ControlNet](https://huggingface.co/lllyasviel/ControlNet/blob/main/models/control_sd15_openpose.pth)

4. Place the models in the following directories:
    ```
    models/StableDiffusion/
    models/ControlNet/
    ```

### Usage
1. Place your input video in the `input/` directory.
2. Run the wrapper script:
    ```bash
    python comfyui_wrapper.py --input_video_path input/input_video.mp4 --output_video_path output/generated_video.mp4
    ```

3. The generated video will be saved in the `output/` directory.

